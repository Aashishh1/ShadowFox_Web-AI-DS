{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c8dcdb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using BERT (IMDb Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50070066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341c1a7",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5325341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = {\n",
    "        'text': [\n",
    "            \"This movie was fantastic and thrilling!\",\n",
    "            \"Absolutely boring and predictable.\",\n",
    "            \"A masterpiece of storytelling.\",\n",
    "            \"Terrible acting and poor script.\"\n",
    "        ],\n",
    "        'label': [1, 0, 1, 0]  # 1: positive, 0: negative\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876e98d",
   "metadata": {},
   "source": [
    "## 2. BERT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985f0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in df['text']:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df['label'].values)\n",
    "    \n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "def train_model(model, train_dataloader, epochs=3):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a349ec",
   "metadata": {},
   "source": [
    "## 3. Analysis and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f21cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions, target_names=['Negative', 'Positive'])\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a602125",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eafd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(tokenizer, model, text, max_len=128):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    attention = outputs.attentions[-1][0, 0].detach().numpy()  # Last layer, first head\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
    "    plt.title(\"BERT Attention Weights\")\n",
    "    plt.savefig('attention_heatmap.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de8cda",
   "metadata": {},
   "source": [
    "## 5. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08b298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_data()\n",
    "    input_ids, attention_masks, labels = preprocess_data(df, tokenizer)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(\"Training BERT model...\")\n",
    "    train_model(model, train_dataloader)\n",
    "    accuracy, report = evaluate_model(model, test_dataloader)\n",
    "    \n",
    "    # Display results\n",
    "    display(HTML(f\"<h3>Model Accuracy: {accuracy:.4f}</h3>\"))\n",
    "    display(HTML(\"<h3>Classification Report:</h3><pre>\" + report + \"</pre>\"))\n",
    "    \n",
    "    # Visualize attention for a sample text\n",
    "    sample_text = \"This movie was fantastic and thrilling!\"\n",
    "    visualize_attention(tokenizer, model, sample_text)\n",
    "    display(HTML(\"<h3>Attention Heatmap Saved as 'attention_heatmap.png'</h3>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3be5d",
   "metadata": {},
   "source": [
    "## 6. Research Questions and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bbabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_questions():\n",
    "    questions = \"\"\"\n",
    "    ### Research Questions:\n",
    "    1. **Contextual Understanding**: How effectively does BERT capture contextual relationships in short versus long text inputs?\n",
    "    2. **Creativity**: Can BERT generate creative text outputs, or is it primarily suited for classification tasks?\n",
    "    3. **Domain Adaptability**: How well does BERT adapt to domain-specific tasks with limited fine-tuning?\n",
    "    4. **Limitations**: What are the computational and data requirements for BERT to perform optimally?\n",
    "    \n",
    "    ### Observations:\n",
    "    - BERT excels in understanding context due to its bidirectional architecture, as seen in the attention heatmap.\n",
    "    - It is less suited for creative text generation compared to models like GPT-3.\n",
    "    - Fine-tuning on small datasets can lead to overfitting, requiring careful hyperparameter tuning.\n",
    "    - High computational cost limits accessibility for low-resource environments.\n",
    "    \"\"\"\n",
    "    display(HTML(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29566d6d",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1b9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion():\n",
    "    insights = \"\"\"\n",
    "    ### Conclusion:\n",
    "    This project demonstrated BERT's capabilities in sentiment analysis, highlighting its strength in contextual understanding through bidirectional processing. The attention visualization revealed how BERT focuses on relevant tokens, enhancing interpretability. However, its computational demands and limited creative generation capabilities suggest opportunities for improvement in efficiency and versatility. Potential applications include automated sentiment analysis in social media or customer feedback systems. Future work could explore lightweight BERT variants or hybrid models combining BERT's contextual strengths with generative capabilities.\n",
    "    \n",
    "    ### Ethical Considerations:\n",
    "    - **Bias**: BERT may inherit biases from training data, requiring careful monitoring in sensitive applications.\n",
    "    - **Accessibility**: High resource demands limit its use in low-resource settings, raising equity concerns.\n",
    "    - **Transparency**: Attention visualizations improve interpretability, aligning with ethical AI practices.\n",
    "    \"\"\"\n",
    "    display(HTML(insights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41674e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab9779fdc4c4a559965aff2eb3e8c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashish Mishra\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ashish Mishra\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b31e787e56c49b89d4a75081dd5c7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36c7389b5774e1b9752bd6c2bd3841c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4825bed92cb24222bb199f6efd315150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5963fd62af504c6184974caf86f7b8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Ashish Mishra\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT model...\n",
      "Epoch 1, Loss: 0.6280363649129868\n",
      "Epoch 2, Loss: 0.5016179233789444\n",
      "Epoch 3, Loss: 0.4960475564002991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashish Mishra\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ashish Mishra\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ashish Mishra\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ashish Mishra\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ashish Mishra\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ashish Mishra\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Model Accuracy: 0.0000</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Classification Report:</h3><pre>              precision    recall  f1-score   support\n",
       "\n",
       "    Negative       0.00      0.00      0.00       1.0\n",
       "    Positive       0.00      0.00      0.00       0.0\n",
       "\n",
       "    accuracy                           0.00       1.0\n",
       "   macro avg       0.00      0.00      0.00       1.0\n",
       "weighted avg       0.00      0.00      0.00       1.0\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Attention Heatmap Saved as 'attention_heatmap.png'</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    ### Research Questions:\n",
       "    1. **Contextual Understanding**: How effectively does BERT capture contextual relationships in short versus long text inputs?\n",
       "    2. **Creativity**: Can BERT generate creative text outputs, or is it primarily suited for classification tasks?\n",
       "    3. **Domain Adaptability**: How well does BERT adapt to domain-specific tasks with limited fine-tuning?\n",
       "    4. **Limitations**: What are the computational and data requirements for BERT to perform optimally?\n",
       "    \n",
       "    ### Observations:\n",
       "    - BERT excels in understanding context due to its bidirectional architecture, as seen in the attention heatmap.\n",
       "    - It is less suited for creative text generation compared to models like GPT-3.\n",
       "    - Fine-tuning on small datasets can lead to overfitting, requiring careful hyperparameter tuning.\n",
       "    - High computational cost limits accessibility for low-resource environments.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    ### Conclusion:\n",
       "    This project demonstrated BERT's capabilities in sentiment analysis, highlighting its strength in contextual understanding through bidirectional processing. The attention visualization revealed how BERT focuses on relevant tokens, enhancing interpretability. However, its computational demands and limited creative generation capabilities suggest opportunities for improvement in efficiency and versatility. Potential applications include automated sentiment analysis in social media or customer feedback systems. Future work could explore lightweight BERT variants or hybrid models combining BERT's contextual strengths with generative capabilities.\n",
       "    \n",
       "    ### Ethical Considerations:\n",
       "    - **Bias**: BERT may inherit biases from training data, requiring careful monitoring in sensitive applications.\n",
       "    - **Accessibility**: High resource demands limit its use in low-resource settings, raising equity concerns.\n",
       "    - **Transparency**: Attention visualizations improve interpretability, aligning with ethical AI practices.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    research_questions()\n",
    "    conclusion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
